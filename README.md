# Ijon - Intelligent PDF RAG System with Gemini Embeddings

An advanced Retrieval-Augmented Generation (RAG) system for extracting and querying knowledge from PDF documents, featuring Google's Gemini embeddings and smart contextual enhancement.

## ğŸ¯ Overview

Ijon processes PDF documents (particularly books on mental models, psychology, and decision-making) into a searchable knowledge base using:

- **Gemini text-embedding-004** for semantic embeddings (768 dimensions)
- **Neon PostgreSQL with pgvector** for vector storage and similarity search
- **Smart Context Enhancement** for improved retrieval quality
- **Mental Model Detection** for domain-specific understanding

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone [repository-url]
cd Ijon

# 2. Set up environment
python -m venv venv_ijon
source venv_ijon/bin/activate  # On Windows: venv_ijon\Scripts\activate
pip install -r requirements.txt

# 3. Configure environment variables
cp .env.example .env
# Edit .env with your API keys:
# - GEMINI_API_KEY
# - NEON_CONNECTION_STRING or DATABASE_URL

# 4. Initialize the database
python setup_neon_database.py
python migrate_neon_for_gemini.py

# 5. Process a PDF
python process_single_book.py path/to/your.pdf

# 6. Query the system
python query_mental_models.py
# Or direct query:
python query_mental_models.py "What is flow state?"
```

## ğŸ“š System Architecture

### Core Pipeline
```
PDF â†’ Text Extraction â†’ Smart Chunking â†’ Context Enhancement â†’ Gemini Embeddings â†’ Neon pgvector
                                                â†“
Query â†’ Context Enhancement â†’ Gemini Embedding â†’ Similarity Search â†’ Ranked Results
```

### Key Components

1. **PDF Processing** (`src/pdf_processor/`)
   - Extracts text from PDFs using PyPDF2/PyMuPDF
   - Handles both text-based and scanned documents
   - Implements semantic chunking with overlap

2. **Smart Context Enhancement** (`src/context/smart_context_enhancer.py`)
   - Detects mental models and concepts
   - Adds contextual metadata before embedding
   - Three enhancement levels: basic, smart, analytical

3. **Gemini Embeddings** (`src/rag/gemini_embedder.py`)
   - Uses Google's text-embedding-004 model
   - 768-dimensional embeddings
   - ~$0.025 per million tokens
   - Optimized for retrieval tasks

4. **Vector Storage** (Neon PostgreSQL)
   - pgvector extension for similarity search
   - Indexed for fast retrieval
   - Supports filtering by metadata

## ğŸ”§ Configuration

### Environment Variables
```bash
# Required
GEMINI_API_KEY=your-gemini-api-key
DATABASE_URL=postgresql://user:pass@host/db  # or NEON_CONNECTION_STRING

# Optional
LOG_LEVEL=INFO
CACHE_DIR=.cache
ENABLE_CACHE=true
```

### Processing Options
```python
# Limit pages for large PDFs
python process_single_book.py book.pdf --max-pages 100

# Process with custom context level
python process_single_book.py book.pdf --context-level 2
```

## ğŸ“Š Current Data

The system currently contains:
- **Flow: The Psychology of Optimal Experience** - Complete (317 pages)
- Additional mental models books can be added via `process_single_book.py`

## ğŸ” Querying

### Interactive Mode
```bash
python query_mental_models.py
```

### Command Line Query
```bash
python query_mental_models.py "How do I achieve flow state?"
```

### Demo Queries
```bash
python query_mental_models.py demo
```

### Python API
```python
from src.rag.gemini_embedder import GeminiEmbeddingGenerator
from src.context.smart_context_enhancer import SmartContextEnhancer
import asyncio

async def query_knowledge(question):
    embedder = GeminiEmbeddingGenerator()
    enhancer = SmartContextEnhancer()
    
    # Enhance query for better retrieval
    enhanced_query = enhancer.add_context(question, metadata, level=1)
    
    # Generate embedding and search
    embedding = await embedder.generate_embeddings([enhanced_query])
    # ... perform vector search
```

## ğŸ§ª Testing

```bash
# Test database connection
python test_neon_connection.py

# Test Gemini embeddings
python test_gemini_embeddings.py

# Test full pipeline
python test_system.py

# Run all tests
python -m pytest tests/
```

## ğŸ“ Project Structure

```
Ijon/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ gemini_embedder.py      # Gemini embedding generation
â”‚   â”‚   â””â”€â”€ pipeline.py             # Main RAG pipeline
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â””â”€â”€ smart_context_enhancer.py # Context enhancement
â”‚   â”œâ”€â”€ pdf_processor/              # PDF processing modules
â”‚   â””â”€â”€ utils/                      # Utilities and helpers
â”œâ”€â”€ extraction/
â”‚   â””â”€â”€ v2/                         # Experimental 12-factor extraction
â”œâ”€â”€ query_mental_models.py          # Main query interface
â”œâ”€â”€ process_single_book.py          # PDF processing script
â”œâ”€â”€ setup_neon_database.py          # Database setup
â””â”€â”€ requirements.txt                # Dependencies
```

## ğŸš§ Experimental Features

The following features are in development:
- **Cognitive RAG Pipeline** - Agent-based query routing for complex tasks
- **12-Factor Extraction** - Quality-driven extraction with human validation
- **Knowledge Graph Integration** - Entity and relationship extraction

## ğŸ“ˆ Performance

- **Embedding Generation**: ~1-2 seconds per page
- **Query Response**: 50-500ms for semantic search
- **Storage**: ~9.5KB per embedding (when transferred as text)
- **Quality**: 85%+ relevance for domain-specific queries

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow existing code patterns (see CLAUDE.md)
4. Add tests for new functionality
5. Submit a Pull Request

## ğŸ“ License

[Your License Here]

## ğŸ™ Acknowledgments

- Google Gemini for embeddings API
- Neon for serverless PostgreSQL
- pgvector for vector similarity search